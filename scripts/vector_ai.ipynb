{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector-stabilized CDS ↔ Transition Tilt inside a Vector-like Layer\n",
    "\n",
    "This notebook demonstrates a practical credit-risk pattern:\n",
    "\n",
    "1. Link **CDS information** to a **rating transition model** by fitting a scalar tilt parameter $\\alpha$ that scales a baseline generator.\n",
    "2. Stabilize that fit using **vectors/embeddings** (peer prior + shrinkage).\n",
    "3. Wrap the above inside a **Vector-like automation layer** that routes inbound text, retrieves precedents, enforces exception controls, and reuses last good calibration when market data is missing.\n",
    "\n",
    "The goal is to show, end-to-end, how “vector” methods help a bank when CDS observations are noisy/sparse and when workflow automation is required.\n",
    "\n",
    "---\n",
    "\n",
    "## Per-message processing pipeline\n",
    "\n",
    "For each inbound message (email/ticket/chat):\n",
    "\n",
    "1. **Route** the message to a workflow using text embeddings (trade capture vs calibration vs tilt update vs exception).\n",
    "2. **Extract** structured fields (issuer, tenor, spread, currency).\n",
    "3. **Retrieve** similar historical cases (precedents) using embeddings.\n",
    "4. **Control**: if routed to an exception workflow, open a case and **do not update** risk parameters.\n",
    "5. Otherwise:\n",
    "   - if a spread is present, compute a CDS-implied default proxy and calibrate $\\alpha$,\n",
    "   - if a spread is missing, reuse the last good issuer calibration; if none exists, fall back to rating mean (naive) or peer prior (vector).\n",
    "\n",
    "---\n",
    "\n",
    "## Transition model: CTMC generator and default probability\n",
    "\n",
    "We represent credit migration with a continuous-time Markov chain (CTMC) over rating states. Let $Q$ be the generator matrix. In this demo we use four states:\n",
    "\n",
    "- $0 =$ IG  \n",
    "- $1 =$ BBB-ish  \n",
    "- $2 =$ HY  \n",
    "- $3 =$ Default (absorbing)\n",
    "\n",
    "Given a generator $Q$, the transition probability matrix over horizon $T$ is:\n",
    "$\n",
    "P(T) = \\exp(QT)\n",
    "$.\n",
    "If the issuer starts in rating state $r$, the transition-model default probability by time $T$ is:\n",
    "$\n",
    "PD_{\\mathrm{TM}}(T \\mid r) = P(T)_{r, D}\n",
    "$.\n",
    "The code computes $P(T)$ via an eigen-decomposition-based matrix exponential (sufficient for a small $4\\times 4$ demo).\n",
    "\n",
    "---\n",
    "\n",
    "## Linking CDS risk to transitions via a scalar tilt $\\alpha$\n",
    "\n",
    "To link market-implied default risk to the transition model, we scale a baseline generator $Q_{\\mathrm{base}}$:\n",
    "$\n",
    "Q_{\\mathrm{issuer}} = \\alpha \\, Q_{\\mathrm{base}}\n",
    "$.\n",
    "Intuition:\n",
    "- Larger $\\alpha$ increases migration intensities proportionally (including default intensity).\n",
    "- Smaller $\\alpha$ slows migration and reduces default probability.\n",
    "\n",
    "For any horizon $T$ and start rating $r$:\n",
    "$\n",
    "PD_{\\mathrm{TM}}(T \\mid r,\\alpha) = \\left(\\exp(\\alpha Q_{\\mathrm{base}} T)\\right)_{r,D}\n",
    "$.\n",
    "Calibration chooses $\\alpha$ such that $PD_{\\mathrm{TM}}(T)$ matches a market-implied target derived from CDS.\n",
    "\n",
    "---\n",
    "\n",
    "## CDS proxy used in this notebook (simplified)\n",
    "\n",
    "A full CDS calibration bootstraps a term structure of hazard rates $\\lambda(t)$ using multiple maturities and discounting assumptions.\n",
    "\n",
    "To keep the demo self-contained, we use a simplified approximation:\n",
    "\n",
    "- Convert spread from bps to decimal: $s = \\frac{\\text{bp}}{10000}$.\n",
    "- Assume constant recovery $R$.\n",
    "- Use the heuristic relationship:\n",
    "$\n",
    "s \\approx (1-R)\\lambda\n",
    "\\quad\\Rightarrow\\quad\n",
    "\\lambda \\approx \\frac{s}{1-R}\n",
    "$.\n",
    "- Assuming constant hazard $\\lambda$, compute a single-horizon default proxy:\n",
    "$\n",
    "PD_{\\mathrm{CDS}}(T) \\approx 1 - e^{-\\lambda T}\n",
    "$.\n",
    "This $PD_{\\mathrm{CDS}}(T)$ is the target used for fitting $\\alpha$.\n",
    "\n",
    "---\n",
    "\n",
    "## Naive calibration of $\\alpha$ (pure market fit)\n",
    "\n",
    "Given $PD_{\\mathrm{CDS}}(T)$ and start rating $r$, the naive approach fits $\\alpha$ by minimizing squared error:\n",
    "$\n",
    "\\alpha_{\\mathrm{naive}} = \\arg\\min_{\\alpha}\n",
    "\\left(PD_{\\mathrm{TM}}(T \\mid r,\\alpha) - PD_{\\mathrm{CDS}}(T)\\right)^2\n",
    "$.\n",
    "In the code, this is solved via a grid search over $\\alpha \\in [0.01, 5]$.\n",
    "\n",
    "Strength: market-consistent when quotes are reliable.  \n",
    "Weakness: unstable when quotes are noisy/illiquid or missing.\n",
    "\n",
    "---\n",
    "\n",
    "## “Vector” for risk stabilization: issuer embeddings and peer prior\n",
    "\n",
    "Here, “vector” means a numerical representation of issuer state that supports similarity-based inference.\n",
    "\n",
    "Each issuer $i$ has a structured feature vector $x_i$ (e.g., sector, leverage, equity vol, liquidity proxy, rating). After normalization we obtain an embedding $z_i$.\n",
    "\n",
    "Similarity is measured using cosine similarity:\n",
    "$$\n",
    "\\mathrm{sim}(i,j) = \\frac{z_i^\\top z_j}{\\|z_i\\|\\|z_j\\|}\n",
    "$$\n",
    "We compute an issuer-specific peer prior for $\\alpha$ using the $k$ nearest neighbors:\n",
    "$$\n",
    "\\alpha_{\\mathrm{prior}}(i) = \\sum_{j \\in \\mathcal{N}_k(i)} w_{ij}\\,\\alpha_j\n",
    "$$\n",
    "with weights defined by a similarity softmax:\n",
    "$$\n",
    "w_{ij} =\n",
    "\\frac{\\exp(\\mathrm{sim}(i,j)/\\tau)}\n",
    "{\\sum_{\\ell \\in \\mathcal{N}_k(i)} \\exp(\\mathrm{sim}(i,\\ell)/\\tau)}\n",
    "$$\n",
    "This prior is most valuable when market quotes are missing or low quality.\n",
    "\n",
    "---\n",
    "\n",
    "## Vector-stabilized calibration (MAP shrinkage)\n",
    "\n",
    "When a market observation exists, we combine:\n",
    "- fit-to-market, and\n",
    "- shrinkage toward the peer prior.\n",
    "\n",
    "We solve:\n",
    "$$\n",
    "\\alpha_{\\mathrm{vector}} = \\arg\\min_{\\alpha}\n",
    "\\left(PD_{\\mathrm{TM}}(T \\mid r,\\alpha) - PD_{\\mathrm{CDS}}(T)\\right)^2\n",
    "+ \\lambda\\left(\\alpha - \\alpha_{\\mathrm{prior}}(i)\\right)^2\n",
    "$$\n",
    "Interpretation:\n",
    "- The first term enforces market consistency.\n",
    "- The second term prevents extreme/unstable parameters by pulling toward peers.\n",
    "\n",
    "To ensure “market-respecting” behavior, the implementation can constrain $\\alpha$ to stay within a band around the pure market fit $\\alpha_{\\mathrm{mkt}}$, e.g.\n",
    "$$\n",
    "\\alpha \\in \\left[\\alpha_{\\mathrm{mkt}}(1-\\delta),\\, \\alpha_{\\mathrm{mkt}}(1+\\delta)\\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Text embeddings: routing and precedent retrieval (Vector-like layer)\n",
    "\n",
    "Separately from issuer vectors, we embed message text for:\n",
    "\n",
    "1. **Workflow routing**: pick the workflow whose description embedding is most similar to the inbound message.\n",
    "2. **Precedent retrieval**: retrieve similar historical cases for context/governance.\n",
    "\n",
    "With normalized embeddings, inner product corresponds to cosine similarity. If $e$ is the embedding of the inbound message and $v_k$ is the embedding of workflow $k$:\n",
    "$$\n",
    "\\mathrm{score}(k) = e^\\top v_k\n",
    "$$\n",
    "and the selected workflow is:\n",
    "$$\n",
    "k^* = \\arg\\max_k \\mathrm{score}(k)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Controls and governance logic\n",
    "\n",
    "Two production-relevant controls are implemented:\n",
    "\n",
    "### 1) Exception gating\n",
    "If the message routes to the exception workflow (e.g., suspected bad tick), we open a case and **do not update** $\\alpha$.\n",
    "\n",
    "This prevents contaminating calibration/risk parameters with anomalous inputs.\n",
    "\n",
    "### 2) Issuer-level memory for missing quotes\n",
    "If the spread is missing, we reuse the **last calibrated** $\\alpha$ for that issuer (if available). This avoids discontinuities from falling back to rating means or peers when the system simply lacks a fresh quote.\n",
    "\n",
    "Operationally, this mirrors a common desk approach: “carry forward last good curve/tilt unless we have validated new market information.”\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthewhosseini/Documents/PycharmProjects/PortfolioAnalysis/lib/python3.8/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2025-12-19 15:29:40.006589: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from typing import Optional, Dict, List, Tuple, Set\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Reduce TensorFlow verbosity if it gets imported transitively by transformers/sentence-transformers.\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "# Silence tqdm autonotebook warning in some notebook-like environments.\n",
    "warnings.filterwarnings(\"ignore\", message=\"Using `tqdm.autonotebook.tqdm`.*\")\n",
    "\n",
    "# Rating state indices\n",
    "DEFAULT_STATE = 3  # 0=IG, 1=BBB-ish, 2=HY, 3=Default (absorbing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Section 0A — Ratings transition model and continuous-time generator\n",
    "\n",
    "### Objective\n",
    "Construct a baseline *continuous-time* credit migration model (with default as an absorbing state) that can generate multi-horizon transition probabilities and default probabilities, later “tilted” by a scalar factor.\n",
    "\n",
    "### Mathematical framework\n",
    "\n",
    "#### Discrete-time transition matrix\n",
    "Let $P \\in \\mathbb{R}^{K\\times K}$ be a one-period transition matrix across rating states, with rows summing to 1. Default is absorbing:\n",
    "- States: $0=\\text{IG}$, $1=\\text{BBB-ish}$, $2=\\text{HY}$, $3=\\text{Default}$\n",
    "- Absorbing default: $P_{3,3}=1$ and $P_{3,j}=0$ for $j\\neq 3$\n",
    "\n",
    "#### Cohort simulation and empirical estimation\n",
    "A cohort simulation generates rating paths $\\{X_{n,t}\\}$ for names $n=1,\\dots,N$ and time steps $t=0,\\dots,T-1$.\n",
    "\n",
    "Transition counts are computed as:\n",
    "$$\n",
    "n_{ij} = \\sum_{n=1}^{N}\\sum_{\\tau=1}^{T-1}\\mathbf{1}\\{X_{n,\\tau-1}=i,\\;X_{n,\\tau}=j\\}.\n",
    "$$\n",
    "\n",
    "The cohort transition matrix estimator is:\n",
    "$$\n",
    "\\hat P_{ij} = \\frac{n_{ij}}{\\sum_{k=1}^{K} n_{ik}},\n",
    "$$\n",
    "with absorbing default enforced explicitly.\n",
    "\n",
    "#### Continuous-time generator\n",
    "For a continuous-time Markov chain with generator matrix $Q$, the transition matrix over horizon $t$ is:\n",
    "$$\n",
    "P(t) = e^{Qt}.\n",
    "$$\n",
    "\n",
    "The script approximates $Q \\approx \\log(P)$ via a truncated series expansion:\n",
    "$$\n",
    "\\log(P) = (P-I) - \\frac{(P-I)^2}{2} + \\frac{(P-I)^3}{3} - \\cdots\n",
    "$$\n",
    "\n",
    "Because truncation and estimation noise can produce invalid generators, the script applies a “repair”:\n",
    "- Set negative off-diagonals to zero: $Q_{ij}\\leftarrow \\max(Q_{ij},0)$ for $i\\neq j$\n",
    "- Enforce row-sum-zero property:\n",
    "$$\n",
    "Q_{ii} = -\\sum_{j\\neq i} Q_{ij}.\n",
    "$$\n",
    "\n",
    "#### Transition-model-implied default probability\n",
    "Given base generator $Q_{\\text{base}}$, scalar tilt $\\alpha>0$, horizon $T$, and current rating state $r$:\n",
    "$$\n",
    "P_{\\alpha}(T) = e^{(\\alpha Q_{\\text{base}})T},\n",
    "\\qquad\n",
    "\\mathrm{PD}_{\\mathrm{TM}}(T \\mid r,\\alpha) = [P_{\\alpha}(T)]_{r,\\text{Default}}.\n",
    "$$\n",
    "\n",
    "### What the code does / produces\n",
    "- Produces a baseline generator $Q_{\\text{base}}$ from either:\n",
    "  - a simulated-and-estimated transition matrix $\\hat P$, or\n",
    "  - the invented matrix $P_{\\text{true}}$\n",
    "- Provides a function $\\mathrm{PD}_{\\mathrm{TM}}(T \\mid r,\\alpha)$ to map a tilt $\\alpha$ into multi-horizon default probabilities.\n",
    "\n",
    "### Why it is designed this way\n",
    "- The transition matrix provides a *structural* baseline for credit migration.\n",
    "- The generator representation enables coherent multi-horizon behavior via matrix exponentiation rather than ad hoc scaling.\n",
    "- The scalar $\\alpha$ provides a simple one-parameter “tilt” knob to align the structural model to market-implied default probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 0A — Transition matrices + Generator (CTMC)\n",
    "# ============================================================\n",
    "\n",
    "def Invented_Transition_Matrix_4() -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Toy 4-state one-period transition matrix with Default absorbing.\n",
    "    Rows sum to 1, and last row is [0,0,0,1].\n",
    "    \"\"\"\n",
    "    return np.array(\n",
    "        [\n",
    "            [0.9000, 0.0800, 0.0199, 0.0001],\n",
    "            [0.0500, 0.8500, 0.0900, 0.0100],\n",
    "            [0.0100, 0.0900, 0.8000, 0.1000],\n",
    "            [0.0000, 0.0000, 0.0000, 1.0000],\n",
    "        ],\n",
    "        dtype=float,\n",
    "    )\n",
    "\n",
    "\n",
    "def Initialize_Counterparties(num_names: int, start_weights: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Sample initial rating states for num_names using categorical weights start_weights.\n",
    "\n",
    "    start_weights is assumed to be length-4 in this demo (IG, BBB-ish, HY, Default).\n",
    "    \"\"\"\n",
    "    start_state = np.zeros(num_names, dtype=int)\n",
    "    cum_weights = np.cumsum(start_weights)\n",
    "    uniforms = np.random.uniform(0.0, 1.0, num_names)\n",
    "\n",
    "    for n in range(num_names):\n",
    "        if 0.0 < uniforms[n] <= cum_weights[0]:\n",
    "            start_state[n] = 0\n",
    "        elif cum_weights[0] < uniforms[n] <= cum_weights[1]:\n",
    "            start_state[n] = 1\n",
    "        elif cum_weights[1] < uniforms[n] <= cum_weights[2]:\n",
    "            start_state[n] = 2\n",
    "        else:\n",
    "            start_state[n] = 3\n",
    "    return start_state\n",
    "\n",
    "\n",
    "def Transition_Step(current_state: int, transition_matrix: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    One-step draw from the categorical distribution in transition_matrix[current_state, :].\n",
    "    \"\"\"\n",
    "    cum_probs = np.cumsum(transition_matrix[current_state, :])\n",
    "    u = np.random.uniform(0.0, 1.0)\n",
    "    for j in range(transition_matrix.shape[1]):\n",
    "        if u <= cum_probs[j]:\n",
    "            return j\n",
    "    return transition_matrix.shape[1] - 1\n",
    "\n",
    "\n",
    "def Simulate_Rating_Data(\n",
    "    num_names: int,\n",
    "    num_periods: int,\n",
    "    transition_matrix: np.ndarray,\n",
    "    start_weights: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Simulate discrete-time rating paths:\n",
    "      data[n, t] = rating state for name n at time t.\n",
    "    \"\"\"\n",
    "    start_state = Initialize_Counterparties(num_names, start_weights)\n",
    "    data = np.zeros((num_names, num_periods), dtype=int)\n",
    "\n",
    "    for n in range(num_names):\n",
    "        data[n, 0] = start_state[n]\n",
    "        for t in range(1, num_periods):\n",
    "            data[n, t] = Transition_Step(data[n, t - 1], transition_matrix)\n",
    "    return data\n",
    "\n",
    "\n",
    "def Get_Transition_Count(num_states: int, num_periods: int, num_names: int, data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute cohort transition counts:\n",
    "      count[i, j] = number of i->j transitions observed across all names and time steps.\n",
    "    \"\"\"\n",
    "    count_ij = np.zeros((num_states, num_states), dtype=float)\n",
    "    for n in range(num_names):\n",
    "        for tau in range(1, num_periods):\n",
    "            i = int(data[n, tau - 1])\n",
    "            j = int(data[n, tau])\n",
    "            count_ij[i, j] += 1.0\n",
    "    return count_ij\n",
    "\n",
    "\n",
    "def Estimate_Cohort_Transition_Matrix(num_states: int, count_ij: np.ndarray, period_power: int = 1) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Estimate a one-period transition matrix by row-normalizing counts, then enforce absorbing default.\n",
    "\n",
    "    If a row has no observations, fall back to identity on that row.\n",
    "    period_power allows compounding to multi-period transitions via matrix power.\n",
    "    \"\"\"\n",
    "    est_matrix = np.zeros((num_states, num_states), dtype=float)\n",
    "\n",
    "    for i in range(num_states):\n",
    "        denom = np.sum(count_ij[i, :])\n",
    "        if denom > 0:\n",
    "            est_matrix[i, :] = count_ij[i, :] / denom\n",
    "        else:\n",
    "            est_matrix[i, i] = 1.0\n",
    "\n",
    "    # Enforce absorbing default explicitly (model governance / sanity constraint)\n",
    "    est_matrix[-1, :] = 0.0\n",
    "    est_matrix[-1, -1] = 1.0\n",
    "\n",
    "    return np.linalg.matrix_power(est_matrix, period_power)\n",
    "\n",
    "\n",
    "def Find_Generator_Matrix_Series(transition_matrix: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Approximate generator Q ≈ log(P) using a truncated series expansion around I:\n",
    "      log(P) = (P-I) - (P-I)^2/2 + (P-I)^3/3 - ...\n",
    "\n",
    "    IMPORTANT:\n",
    "    - This truncation does NOT guarantee a valid generator.\n",
    "    - Empirical P may also violate embeddability (there may not exist a valid Q with exp(Q)=P).\n",
    "\n",
    "    The code therefore \"repairs\" Q:\n",
    "    - Clip negative off-diagonal entries to 0\n",
    "    - Recompute diagonals to enforce row-sum approximately 0\n",
    "\n",
    "    Returns:\n",
    "      tilde_q: raw series approximation\n",
    "      q_repaired: repaired generator suitable for CTMC usage in this demo\n",
    "    \"\"\"\n",
    "    num_states = transition_matrix.shape[0]\n",
    "    delta = transition_matrix - np.eye(num_states)\n",
    "\n",
    "    tilde_q = delta.copy()\n",
    "    term = delta.copy()\n",
    "\n",
    "    # Truncate at k = num_states + 1 terms (toy choice; adequate for demo scale, not production-grade)\n",
    "    for k in range(2, num_states + 1):\n",
    "        term = -np.dot(term, delta) / float(k)\n",
    "        tilde_q += term\n",
    "\n",
    "    q_repaired = tilde_q.copy()\n",
    "\n",
    "    # Repair step: enforce Q_{ij} >= 0 for i != j and row sums ~ 0\n",
    "    for i in range(num_states):\n",
    "        for j in range(num_states):\n",
    "            if i != j and q_repaired[i, j] < 0.0:\n",
    "                q_repaired[i, j] = 0.0\n",
    "\n",
    "        # Set diagonal so row sums to 0: Q_{ii} = -sum_{j!=i} Q_{ij}\n",
    "        off_diag_sum = np.sum(q_repaired[i, :]) - q_repaired[i, i]\n",
    "        q_repaired[i, i] = -off_diag_sum\n",
    "\n",
    "    return tilde_q, q_repaired\n",
    "\n",
    "\n",
    "def Expm_Via_Eig(matrix_a: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Matrix exponential via eigen-decomposition.\n",
    "    Appropriate for small matrices (4x4 demo). For production, prefer scipy.linalg.expm or scaling-squaring.\n",
    "\n",
    "    exp(A) = V exp(diag(w)) V^{-1}\n",
    "    \"\"\"\n",
    "    eigen_vals, eigen_vecs = np.linalg.eig(matrix_a)\n",
    "    inv_eigen_vecs = np.linalg.inv(eigen_vecs)\n",
    "    return (eigen_vecs @ np.diag(np.exp(eigen_vals)) @ inv_eigen_vecs).real\n",
    "\n",
    "\n",
    "def Pd_Tm(generator_base: np.ndarray, alpha: float, horizon_years: float, rating_state: int) -> float:\n",
    "    \"\"\"\n",
    "    Transition-model default probability over horizon_years given:\n",
    "      Q_issuer = alpha * generator_base\n",
    "      P(T) = exp(Q_issuer * T)\n",
    "\n",
    "    Returns P(T)[rating_state, DEFAULT_STATE].\n",
    "    \"\"\"\n",
    "    trans_matrix = Expm_Via_Eig((alpha * generator_base) * horizon_years)\n",
    "    return float(trans_matrix[rating_state, DEFAULT_STATE])\n",
    "\n",
    "\n",
    "def Build_Q_Base_From_Transition_Data(\n",
    "    seed: int = 7,\n",
    "    use_estimated: bool = True,\n",
    "    num_names: int = 1000,\n",
    "    num_periods_years: int = 10,\n",
    "    start_weights: np.ndarray = np.array([0.45, 0.35, 0.20, 0.00]),\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Either:\n",
    "      - simulate cohort paths from a 'true' P, estimate P_hat, then compute Q_base from P_hat, OR\n",
    "      - compute Q_base directly from the true P (for deterministic demo).\n",
    "\n",
    "    Returns:\n",
    "      p_used: transition matrix used (estimated or true)\n",
    "      q_base: repaired CTMC generator used downstream\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    p_true = Invented_Transition_Matrix_4()\n",
    "\n",
    "    if use_estimated:\n",
    "        data = Simulate_Rating_Data(num_names=num_names, num_periods=num_periods_years, transition_matrix=p_true, start_weights=start_weights)\n",
    "        count_ij = Get_Transition_Count(num_states=4, num_periods=num_periods_years, num_names=num_names, data=data)\n",
    "        p_hat = Estimate_Cohort_Transition_Matrix(num_states=4, count_ij=count_ij, period_power=1)\n",
    "        _, q_base = Find_Generator_Matrix_Series(p_hat)\n",
    "        return p_hat, q_base\n",
    "\n",
    "    _, q_base = Find_Generator_Matrix_Series(p_true)\n",
    "    return p_true, q_base\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0B — CDS par spread pricing and piecewise-constant hazard calibration\n",
    "\n",
    "### Objective\n",
    "Infer a *hazard rate term structure* from observed CDS spreads, then convert that hazard curve into market-implied default probabilities at given horizons.\n",
    "\n",
    "### Mathematical framework\n",
    "\n",
    "#### Discounting\n",
    "A flat annual discount curve is used:\n",
    "$$\n",
    "DF(0,t) = (1+r)^{-t}.\n",
    "$$\n",
    "\n",
    "#### Survival probability under piecewise-constant hazard\n",
    "Let tenors be $0<T_1<\\cdots<T_m$. Define piecewise-constant hazards $\\gamma_k$ on $(T_{k-1},T_k]$ (with $T_0=0$). The cumulative hazard up to time $t$ is:\n",
    "$$\n",
    "\\Lambda(t) = \\int_0^t \\lambda(u)\\,du\n",
    "          = \\sum_{k=1}^{m}\\gamma_k\\cdot\\bigl(\\min(t,T_k)-T_{k-1}\\bigr)\n",
    "          + \\gamma_m\\cdot\\max(0,t-T_m).\n",
    "$$\n",
    "\n",
    "Survival probability:\n",
    "$$\n",
    "S(t) = e^{-\\Lambda(t)}.\n",
    "$$\n",
    "\n",
    "#### CDS par spread (discrete quarterly approximation)\n",
    "With recovery $R$, the script approximates:\n",
    "- Protection leg (default payment) using survival differences:\n",
    "$$\n",
    "\\mathrm{ProtLeg} \\approx \\sum_{j} DF(0,t_j)\\,[S(t_{j-1})-S(t_j)]\n",
    "$$\n",
    "- Premium leg (spread payments) using expected survival:\n",
    "$$\n",
    "\\mathrm{PremLeg} \\approx \\sum_{j} DF(0,t_j)\\,S(t_j)\\,\\Delta t,\n",
    "\\qquad \\Delta t = 0.25.\n",
    "$$\n",
    "\n",
    "Par spread is:\n",
    "$$\n",
    "s^* = (1-R)\\frac{\\mathrm{ProtLeg}}{\\mathrm{PremLeg}},\n",
    "\\qquad \\text{and reported in bp as } 10{,}000\\,s^*.\n",
    "$$\n",
    "\n",
    "#### Bootstrapping piecewise hazards\n",
    "Given market par spreads $\\{s(T_k)\\}$ at maturities $\\{T_k\\}$, the script bootstraps sequentially:\n",
    "- For each $k$, solve for $\\gamma_k$ such that:\n",
    "$$\n",
    "\\mathrm{ParSpread}\\bigl(\\gamma_1,\\dots,\\gamma_k; T_k\\bigr) = s(T_k),\n",
    "$$\n",
    "using bisection root-finding.\n",
    "\n",
    "#### CDS-implied default probability\n",
    "Once hazards are calibrated, CDS-implied default probability at horizon $T$ is:\n",
    "$$\n",
    "\\mathrm{PD}_{\\mathrm{CDS}}(T) = 1 - S(T) = 1 - e^{-\\Lambda(T)}.\n",
    "$$\n",
    "\n",
    "### What the code does / produces\n",
    "- Parses free text to extract spreads by tenor: $\\{T_k \\mapsto s(T_k)\\}$.\n",
    "- Bootstraps hazards $\\{\\gamma_k\\}$ consistent with those spreads.\n",
    "- Produces CDS-implied PD targets $\\mathrm{PD}_{\\mathrm{CDS}}(T_k)$ at quoted tenors.\n",
    "- If only one tenor is quoted, it also prints a “flat-hazard extrapolation” across reporting horizons.\n",
    "\n",
    "### Why it is designed this way\n",
    "- CDS spreads are a market-consistent source of default risk.\n",
    "- Piecewise-constant hazards are a standard tractable approximation for calibrating term structures.\n",
    "- Bootstrapping ensures each quoted tenor is fit exactly (within numerical tolerance).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 0B — CDS pricing + piecewise-constant hazard bootstrap\n",
    "# ============================================================\n",
    "\n",
    "class Discount_Curve_Base(object):\n",
    "    def Df(self, t1: float, t2: float) -> float:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Flat_Discount_Curve(Discount_Curve_Base):\n",
    "    def __init__(self, rate_r: float):\n",
    "        self.rate_r = float(rate_r)\n",
    "\n",
    "    def Df(self, t1: float, t2: float) -> float:\n",
    "        # Simple annual compounding: DF(0,t)=(1+r)^(-t)\n",
    "        return (1.0 + self.rate_r) ** -(t2 - t1)\n",
    "\n",
    "\n",
    "class Credit_Default_Swap:\n",
    "    def __init__(self, maturity_years: float, discount_curve: Discount_Curve_Base, recovery_r: float = 0.40):\n",
    "        self.maturity_years = float(maturity_years)\n",
    "        self.discount_curve = discount_curve\n",
    "        self.recovery_r = float(recovery_r)\n",
    "\n",
    "    def Payment_Dates(self) -> np.ndarray:\n",
    "        # Quarterly schedule: 0, 0.25, ..., maturity-0.25\n",
    "        return np.arange(0.0, self.maturity_years, 0.25)\n",
    "\n",
    "    def Survival_Probability(self, parameters, t: float) -> float:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def Par_Spread_Bp(self, parameters) -> float:\n",
    "        \"\"\"\n",
    "        Discrete approximation of CDS par spread:\n",
    "          spread = (1-R) * ProtLeg / PremLeg\n",
    "        Returned in basis points.\n",
    "        \"\"\"\n",
    "        dates = self.Payment_Dates()\n",
    "        prot_leg = 0.0\n",
    "        prem_leg = 0.0\n",
    "\n",
    "        for date in dates:\n",
    "            t_start = date\n",
    "            t_end = date + 0.25\n",
    "\n",
    "            df = self.discount_curve.Df(0.0, t_end)\n",
    "            surv_start = self.Survival_Probability(parameters, t_start)\n",
    "            surv_end = self.Survival_Probability(parameters, t_end)\n",
    "\n",
    "            # Protection leg approximated by survival drop over accrual: S(t_start)-S(t_end)\n",
    "            prot_leg += df * (surv_start - surv_end)\n",
    "\n",
    "            # Premium leg approximated by survival at pay date * accrual\n",
    "            prem_leg += df * surv_end * 0.25\n",
    "\n",
    "        spread_decimal = (1.0 - self.recovery_r) * prot_leg / max(prem_leg, 1e-16)\n",
    "        return float(spread_decimal * 10000.0)\n",
    "\n",
    "\n",
    "class Ihp_Credit_Default_Swap(Credit_Default_Swap):\n",
    "    def __init__(self, tenors_years: List[float], **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.tenors_years = list(map(float, tenors_years))\n",
    "\n",
    "    def _Hazard_Integral(self, gammas: List[float], t: float) -> float:\n",
    "        \"\"\"\n",
    "        Piecewise-constant hazard:\n",
    "          lambda(t) = gamma_k for t in (T_{k-1}, T_k]\n",
    "        Then cumulative hazard:\n",
    "          Λ(t)=∑ gamma_k * length_of_overlap\n",
    "        \"\"\"\n",
    "        total = 0.0\n",
    "        prev = 0.0\n",
    "        for idx, tenor in enumerate(self.tenors_years):\n",
    "            gamma = float(gammas[idx])\n",
    "            if t >= tenor:\n",
    "                total += gamma * (tenor - prev)\n",
    "            else:\n",
    "                total += gamma * (t - prev)\n",
    "                return total\n",
    "            prev = tenor\n",
    "\n",
    "        # If t extends beyond last tenor, extend with last gamma.\n",
    "        if t > self.tenors_years[-1]:\n",
    "            total += float(gammas[-1]) * (t - self.tenors_years[-1])\n",
    "\n",
    "        return total\n",
    "\n",
    "    def Survival_Probability(self, gammas: List[float], t: float) -> float:\n",
    "        return float(np.exp(-self._Hazard_Integral(gammas, float(t))))\n",
    "\n",
    "\n",
    "def _Bisect_Root(function_f, lo: float, hi: float, max_iter: int = 80, tol: float = 1e-10) -> float:\n",
    "    \"\"\"\n",
    "    Bisection root finder. If f(lo) and f(hi) have same sign, expands hi up to find a bracket.\n",
    "    \"\"\"\n",
    "    f_lo = function_f(lo)\n",
    "    f_hi = function_f(hi)\n",
    "\n",
    "    if np.sign(f_lo) == np.sign(f_hi):\n",
    "        # Try to find a bracket by expanding hi.\n",
    "        for _ in range(30):\n",
    "            hi *= 2.0\n",
    "            f_hi = function_f(hi)\n",
    "            if np.sign(f_lo) != np.sign(f_hi):\n",
    "                break\n",
    "        else:\n",
    "            # Could not bracket; return lo as safe fallback (demo behavior).\n",
    "            return lo\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        mid = 0.5 * (lo + hi)\n",
    "        f_mid = function_f(mid)\n",
    "\n",
    "        if abs(f_mid) < tol:\n",
    "            return mid\n",
    "\n",
    "        if np.sign(f_mid) == np.sign(f_lo):\n",
    "            lo, f_lo = mid, f_mid\n",
    "        else:\n",
    "            hi, f_hi = mid, f_mid\n",
    "\n",
    "    return 0.5 * (lo + hi)\n",
    "\n",
    "\n",
    "def Parse_Spreads_By_Tenor(message_text: str) -> Dict[float, float]:\n",
    "    \"\"\"\n",
    "    Extract {tenor_years: spread_bp} from free text.\n",
    "    Supports:\n",
    "      - \"1Y=80bp 3Y=105bp 5Y=120bp\"\n",
    "      - \"5Y:120bp\"\n",
    "      - \"5Y CDS @ 120bp\" / \"5Y @ 120bp\"\n",
    "    \"\"\"\n",
    "    spreads_by_tenor: Dict[float, float] = {}\n",
    "\n",
    "    # Pattern A: explicit bindings (1Y=80bp, 3Y:105bp, etc.)\n",
    "    for match in re.finditer(r\"(\\d+)\\s*Y\\s*[:=]\\s*(\\d+(?:\\.\\d+)?)\\s*bp\", message_text, flags=re.IGNORECASE):\n",
    "        spreads_by_tenor[float(match.group(1))] = float(match.group(2))\n",
    "\n",
    "    # Pattern B: tenor ... @ spread\n",
    "    for match in re.finditer(r\"(\\d+)\\s*Y\\b[^0-9]{0,30}@\\s*(\\d+(?:\\.\\d+)?)\\s*bp\", message_text, flags=re.IGNORECASE):\n",
    "        spreads_by_tenor[float(match.group(1))] = float(match.group(2))\n",
    "\n",
    "    return spreads_by_tenor\n",
    "\n",
    "\n",
    "def Calibrate_Piecewise_Hazards(\n",
    "    spreads_by_tenor: Dict[float, float],\n",
    "    discount_curve: Discount_Curve_Base,\n",
    "    recovery_r: float = 0.40,\n",
    ") -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Sequential bootstrap:\n",
    "      For tenor T_k, solve for gamma_k so that model par spread equals market spread at T_k.\n",
    "\n",
    "    Output:\n",
    "      tenors_sorted, gammas\n",
    "    \"\"\"\n",
    "    tenors_sorted = sorted(spreads_by_tenor.keys())\n",
    "    gammas: List[float] = []\n",
    "\n",
    "    for k, tenor_k in enumerate(tenors_sorted):\n",
    "        target_bp = float(spreads_by_tenor[tenor_k])\n",
    "        local_tenors = tenors_sorted[: k + 1]\n",
    "\n",
    "        def objective(gamma_k: float) -> float:\n",
    "            gamma_all = gammas + [float(gamma_k)]\n",
    "            cds = Ihp_Credit_Default_Swap(\n",
    "                tenors_years=local_tenors,\n",
    "                maturity_years=tenor_k,\n",
    "                discount_curve=discount_curve,\n",
    "                recovery_r=recovery_r,\n",
    "            )\n",
    "            return cds.Par_Spread_Bp(gamma_all) - target_bp\n",
    "\n",
    "        gamma_k = _Bisect_Root(objective, lo=1e-8, hi=1.0)\n",
    "        gammas.append(float(gamma_k))\n",
    "\n",
    "    return tenors_sorted, gammas\n",
    "\n",
    "\n",
    "def Cds_Pd_From_Hazards(tenors_years: List[float], gammas: List[float], horizon_years: float) -> float:\n",
    "    \"\"\"\n",
    "    Convert hazards to CDS-implied PD at horizon:\n",
    "      PD(T) = 1 - S(T)\n",
    "    Uses dummy discounting because survival is independent of discounting in this structure.\n",
    "    \"\"\"\n",
    "    dummy_curve = Flat_Discount_Curve(rate_r=0.0)\n",
    "    cds = Ihp_Credit_Default_Swap(\n",
    "        tenors_years=tenors_years,\n",
    "        maturity_years=max(max(tenors_years), horizon_years),\n",
    "        discount_curve=dummy_curve,\n",
    "        recovery_r=0.40,\n",
    "    )\n",
    "    surv = cds.Survival_Probability(gammas, horizon_years)\n",
    "    return float(1.0 - surv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 — Structured issuer vectors and multi-horizon $\\alpha$ estimation (naive vs vector prior)\n",
    "\n",
    "### Objective\n",
    "Estimate a scalar tilt $\\alpha$ such that transition-model default probabilities match CDS-implied PDs, while stabilizing estimates for sparse/low-quality data via a peer-conditioned vector prior.\n",
    "\n",
    "### Mathematical framework\n",
    "\n",
    "#### Structured issuer feature vectors\n",
    "Each issuer $i$ has a feature vector $x_i$ including:\n",
    "- sector one-hot encoding,\n",
    "- leverage, equity volatility, liquidity proxy,\n",
    "- rating state (as a numeric feature)\n",
    "\n",
    "The script standardizes features (z-score):\n",
    "$$\n",
    "z_{i,d} = \\frac{x_{i,d}-\\mu_d}{\\sigma_d+\\epsilon},\n",
    "$$\n",
    "where $(\\mu_d,\\sigma_d)$ are cross-sectional mean/std.\n",
    "\n",
    "#### Similarity and KNN prior on $\\alpha$\n",
    "Cosine similarity between issuer $i$ and observed issuers:\n",
    "$$\n",
    "\\mathrm{sim}(i,j) = \\frac{z_i^\\top z_j}{\\|z_i\\|\\,\\|z_j\\|}.\n",
    "$$\n",
    "\n",
    "Top-$k$ neighbors define weights via a softmax-like rule:\n",
    "$$\n",
    "w_j \\propto \\exp\\Bigl(\\frac{\\mathrm{sim}(i,j)}{\\tau}\\Bigr),\n",
    "\\qquad \\sum_j w_j = 1,\n",
    "$$\n",
    "and prior:\n",
    "$$\n",
    "\\alpha_{\\text{prior}}(i)=\\sum_{j\\in\\mathcal{N}_k(i)} w_j\\,\\alpha_{\\text{anchor}}(j).\n",
    "$$\n",
    "\n",
    "Here $\\alpha_{\\text{anchor}}(j)$ is an “anchor” tilt inferred from a single-horizon observation (5Y PD) for issuers with observed data.\n",
    "\n",
    "#### Market-fit $\\alpha$ via grid search (multi-horizon)\n",
    "Given PD targets at horizons $\\mathcal{T}$, rating state $r$, and model $\\mathrm{PD}_{\\mathrm{TM}}(T\\mid r,\\alpha)$, the naive estimate solves:\n",
    "$$\n",
    "\\alpha_{\\text{mkt}} = \\arg\\min_{\\alpha\\in\\mathcal{G}}\n",
    "\\sum_{T\\in\\mathcal{T}} \\omega_T \\left(\\mathrm{PD}_{\\mathrm{TM}}(T\\mid r,\\alpha) - \\mathrm{PD}_{\\mathrm{CDS}}(T)\\right)^2.\n",
    "$$\n",
    "\n",
    "#### MAP estimate with prior regularization\n",
    "The vector-regularized estimate adds a quadratic penalty:\n",
    "$$\n",
    "\\alpha_{\\text{MAP}} = \\arg\\min_{\\alpha\\in\\mathcal{G}}\n",
    "\\sum_{T\\in\\mathcal{T}} \\omega_T \\left(\\mathrm{PD}_{\\mathrm{TM}}(T\\mid r,\\alpha) - \\mathrm{PD}_{\\mathrm{CDS}}(T)\\right)^2\n",
    "+ \\lambda(\\alpha-\\alpha_{\\text{prior}})^2.\n",
    "$$\n",
    "\n",
    "The script makes $\\lambda$ larger when confidence is lower (stronger pull to the prior).\n",
    "\n",
    "### What the code does / produces\n",
    "- Constructs a synthetic issuer universe with structured features and partially missing observed PDs.\n",
    "- Derives “anchor” $\\alpha$ values for issuers with observed PD(5Y).\n",
    "- Produces two estimators:\n",
    "  - **Naive**: market-fit only, no peer stabilization.\n",
    "  - **Vector**: market-fit plus KNN prior regularization, scaled by a confidence score.\n",
    "- Outputs $\\alpha_{\\text{naive}}$, $\\alpha_{\\text{vector}}$ and corresponding $\\mathrm{PD}_{\\mathrm{TM}}(1Y,3Y,5Y)$.\n",
    "\n",
    "### Why it is designed this way\n",
    "- A single scalar $\\alpha$ is an interpretable “regime tilt” on migration intensity.\n",
    "- Multi-horizon fitting prevents the model from matching one tenor while distorting others.\n",
    "- The vector prior stabilizes estimates when quotes are missing, sparse, or low confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 1 — Structured issuer vectors + alpha estimators\n",
    "# ============================================================\n",
    "\n",
    "def Normalize_Rows(matrix_x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Z-score each column across rows.\n",
    "    NOTE: Despite the name, this normalizes features per column (not row norms).\n",
    "    \"\"\"\n",
    "    mean_vec = matrix_x.mean(axis=0, keepdims=True)\n",
    "    std_vec = matrix_x.std(axis=0, keepdims=True) + 1e-12\n",
    "    return (matrix_x - mean_vec) / std_vec\n",
    "\n",
    "\n",
    "def Cosine_Sim(vector_a: np.ndarray, matrix_b: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between vector_a and each row of matrix_b.\n",
    "    \"\"\"\n",
    "    a_norm = np.linalg.norm(vector_a) + 1e-12\n",
    "    b_norm = np.linalg.norm(matrix_b, axis=1) + 1e-12\n",
    "    return (matrix_b @ vector_a) / (b_norm * a_norm)\n",
    "\n",
    "\n",
    "def Knn_Prior_Alpha(\n",
    "    issuer_embed: np.ndarray,\n",
    "    embeds_obs: np.ndarray,\n",
    "    alpha_obs: np.ndarray,\n",
    "    k: int = 20,\n",
    "    tau: float = 0.12,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    KNN prior:\n",
    "      - find top-k cosine similarities\n",
    "      - compute softmax weights exp(sim/tau)\n",
    "      - return weighted average of observed alpha anchors\n",
    "    \"\"\"\n",
    "    sims = Cosine_Sim(issuer_embed, embeds_obs)\n",
    "    idx = np.argsort(-sims)[:k]\n",
    "    top_sims = sims[idx]\n",
    "\n",
    "    weights = np.exp(top_sims / max(tau, 1e-6))\n",
    "    weights = weights / (weights.sum() + 1e-12)\n",
    "\n",
    "    return float((weights * alpha_obs[idx]).sum())\n",
    "\n",
    "\n",
    "def Build_Structured_Issuer_Universe(generator_base: np.ndarray, seed: int = 7, num_issuers: int = 400) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Create a synthetic issuer universe with:\n",
    "      - categorical sector\n",
    "      - numeric leverage, equity vol, liquidity proxy\n",
    "      - discrete rating state\n",
    "      - partially missing observed PD(5Y) to simulate sparse/illiquid markets\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    issuer_names = [\"ACME\", \"BETA\", \"GAMMA\", \"DELTA\", \"OMEGA\"]\n",
    "    issuer_names += [f\"ISS{i:03d}\" for i in range(num_issuers - len(issuer_names))]\n",
    "\n",
    "    sector = rng.integers(0, 3, size=num_issuers)\n",
    "    sector_one_hot = np.eye(3)[sector]\n",
    "    rating_state = rng.choice([0, 1, 2], size=num_issuers, p=[0.45, 0.35, 0.20])\n",
    "\n",
    "    leverage = rng.normal(loc=2.5, scale=0.7, size=num_issuers).clip(0.5, 6.0)\n",
    "    equity_vol = rng.normal(loc=0.30, scale=0.10, size=num_issuers).clip(0.05, 0.80)\n",
    "    liquidity = rng.normal(loc=1.0, scale=0.4, size=num_issuers).clip(0.1, 3.0)\n",
    "\n",
    "    sector_effect = np.array([0.00, 0.10, 0.25])[sector]\n",
    "    latent = rng.normal(0.0, 1.0, size=num_issuers)\n",
    "\n",
    "    # Synthetic \"true\" alphas; exponential ensures alpha>0\n",
    "    log_alpha_true = -0.1 + 0.35 * (leverage - 2.5) + 0.9 * (equity_vol - 0.30) + 0.25 * latent + sector_effect\n",
    "    alpha_true = np.exp(log_alpha_true).clip(0.01, 5.0)\n",
    "\n",
    "    pd_true_5y = np.array([Pd_Tm(generator_base, alpha_true[i], 5.0, int(rating_state[i])) for i in range(num_issuers)])\n",
    "\n",
    "    # Observation noise increases as liquidity falls (toy)\n",
    "    noise_sd = 0.003 + 0.015 * (1.0 / liquidity)\n",
    "    pd_obs_5y = (pd_true_5y + rng.normal(0.0, noise_sd, size=num_issuers)).clip(1e-5, 0.95)\n",
    "\n",
    "    # Missingness probability increases as liquidity falls\n",
    "    p_missing = (0.10 + 0.55 * (1.0 / (liquidity + 0.2))).clip(0.10, 0.80)\n",
    "    missing_mask = rng.uniform(0.0, 1.0, size=num_issuers) < p_missing\n",
    "    pd_obs_5y[missing_mask] = np.nan\n",
    "\n",
    "    feature_matrix = np.column_stack([sector_one_hot, leverage, equity_vol, liquidity, rating_state.astype(float)])\n",
    "    embeds = Normalize_Rows(feature_matrix)\n",
    "\n",
    "    return {\n",
    "        \"issuer_names\": np.array(issuer_names),\n",
    "        \"rating_state\": rating_state,\n",
    "        \"pd_obs_5y\": pd_obs_5y,\n",
    "        \"embeds\": embeds,\n",
    "    }\n",
    "\n",
    "\n",
    "def Fit_Alpha_From_Pds_Multi(\n",
    "    generator_base: np.ndarray,\n",
    "    pd_targets: Dict[float, float],\n",
    "    rating_state: int,\n",
    "    alpha_grid: np.ndarray,\n",
    "    weights: Optional[Dict[float, float]] = None,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Grid-search alpha to minimize multi-horizon SSE:\n",
    "      sum_T w_T (PD_TM(T|alpha) - PD_target(T))^2\n",
    "    \"\"\"\n",
    "    horizons = sorted(pd_targets.keys())\n",
    "    wts = weights or {h: 1.0 for h in horizons}\n",
    "\n",
    "    sse_list: List[float] = []\n",
    "    for alpha in alpha_grid:\n",
    "        sse = 0.0\n",
    "        for horizon in horizons:\n",
    "            pd_obs = float(np.clip(pd_targets[horizon], 1e-8, 0.999999))\n",
    "            pd_model = Pd_Tm(generator_base, float(alpha), float(horizon), int(rating_state))\n",
    "            sse += float(wts.get(horizon, 1.0)) * (pd_model - pd_obs) ** 2\n",
    "        sse_list.append(float(sse))\n",
    "\n",
    "    sse_arr = np.array(sse_list, dtype=float)\n",
    "    return float(alpha_grid[int(np.argmin(sse_arr))])\n",
    "\n",
    "\n",
    "def Fit_Alpha_Map_Multi(\n",
    "    generator_base: np.ndarray,\n",
    "    pd_targets: Dict[float, float],\n",
    "    rating_state: int,\n",
    "    alpha_prior: float,\n",
    "    alpha_grid: np.ndarray,\n",
    "    lam: float,\n",
    "    weights: Optional[Dict[float, float]] = None,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    MAP estimate via grid search:\n",
    "      sum_T w_T (PD_TM - PD_target)^2 + lam (alpha - alpha_prior)^2\n",
    "    \"\"\"\n",
    "    horizons = sorted(pd_targets.keys())\n",
    "    wts = weights or {h: 1.0 for h in horizons}\n",
    "\n",
    "    obj_list: List[float] = []\n",
    "    for alpha in alpha_grid:\n",
    "        sse = 0.0\n",
    "        for horizon in horizons:\n",
    "            pd_obs = float(np.clip(pd_targets[horizon], 1e-8, 0.999999))\n",
    "            pd_model = Pd_Tm(generator_base, float(alpha), float(horizon), int(rating_state))\n",
    "            sse += float(wts.get(horizon, 1.0)) * (pd_model - pd_obs) ** 2\n",
    "\n",
    "        # Quadratic prior penalty\n",
    "        sse += float(lam) * (float(alpha) - float(alpha_prior)) ** 2\n",
    "        obj_list.append(float(sse))\n",
    "\n",
    "    obj_arr = np.array(obj_list, dtype=float)\n",
    "    return float(alpha_grid[int(np.argmin(obj_arr))])\n",
    "\n",
    "\n",
    "def Build_Alpha_Estimators(generator_base: np.ndarray, universe: Dict[str, np.ndarray]):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      Estimate_Alpha_Naive(issuer, pd_targets) -> alpha\n",
    "      Estimate_Alpha_Vector(issuer, pd_targets, confidence) -> alpha\n",
    "      Get_Rating_State(issuer) -> int\n",
    "      known_issuers -> set[str]\n",
    "    \"\"\"\n",
    "    issuer_names = universe[\"issuer_names\"]\n",
    "    rating_state_arr = universe[\"rating_state\"]\n",
    "    pd_obs_5y = universe[\"pd_obs_5y\"]\n",
    "    embeds = universe[\"embeds\"]\n",
    "\n",
    "    alpha_grid = np.linspace(0.01, 5.0, 800)\n",
    "\n",
    "    # Anchor alphas inferred from PD(5Y) for issuers with observed PD(5Y)\n",
    "    alpha_anchor = np.full(len(issuer_names), np.nan)\n",
    "    for i in range(len(issuer_names)):\n",
    "        if np.isfinite(pd_obs_5y[i]):\n",
    "            alpha_anchor[i] = Fit_Alpha_From_Pds_Multi(\n",
    "                generator_base=generator_base,\n",
    "                pd_targets={5.0: float(pd_obs_5y[i])},\n",
    "                rating_state=int(rating_state_arr[i]),\n",
    "                alpha_grid=alpha_grid,\n",
    "            )\n",
    "\n",
    "    # Naive fallback: rating-mean alpha when no PD targets are available\n",
    "    alpha_rating_mean: Dict[int, float] = {}\n",
    "    for r in [0, 1, 2]:\n",
    "        mask = np.isfinite(alpha_anchor) & (rating_state_arr == r)\n",
    "        alpha_rating_mean[r] = float(np.nanmean(alpha_anchor[mask])) if mask.any() else 1.0\n",
    "\n",
    "    obs_idx = np.where(np.isfinite(alpha_anchor))[0]\n",
    "    embeds_obs = embeds[obs_idx]\n",
    "    alpha_obs_anchor = alpha_anchor[obs_idx]\n",
    "\n",
    "    # Hyperparameters (demo)\n",
    "    base_weights = {1.0: 0.7, 3.0: 0.9, 5.0: 1.0}\n",
    "    lam0 = 0.002\n",
    "    band0 = 0.02\n",
    "    knn_k = 20\n",
    "    softmax_tau = 0.12\n",
    "\n",
    "    issuer_to_idx = {str(n): i for i, n in enumerate(issuer_names)}\n",
    "\n",
    "    def Estimate_Alpha_Naive(issuer: str, pd_targets: Optional[Dict[float, float]]) -> float:\n",
    "        idx = issuer_to_idx.get(issuer, None)\n",
    "        rating_state = int(rating_state_arr[idx]) if idx is not None else 1\n",
    "\n",
    "        if pd_targets:\n",
    "            return Fit_Alpha_From_Pds_Multi(generator_base, pd_targets, rating_state, alpha_grid, weights=base_weights)\n",
    "\n",
    "        return float(alpha_rating_mean[rating_state])\n",
    "\n",
    "    def Estimate_Alpha_Vector(\n",
    "        issuer: str,\n",
    "        pd_targets: Optional[Dict[float, float]],\n",
    "        confidence: float = 1.0,\n",
    "        debug: bool = False,\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Vector-stabilized alpha:\n",
    "          - Compute peer prior alpha_prior via KNN in embedding space\n",
    "          - If PD targets exist: compute market-fit alpha_mkt\n",
    "          - Solve MAP objective with confidence-scaled weights and confidence-scaled regularization strength\n",
    "        \"\"\"\n",
    "        confidence = float(np.clip(confidence, 1e-3, 1.0))\n",
    "\n",
    "        idx = issuer_to_idx.get(issuer, None)\n",
    "        if idx is None:\n",
    "            return float(np.nanmean(alpha_obs_anchor))\n",
    "\n",
    "        rating_state = int(rating_state_arr[idx])\n",
    "        alpha_prior = Knn_Prior_Alpha(embeds[idx], embeds_obs, alpha_obs_anchor, k=knn_k, tau=softmax_tau)\n",
    "\n",
    "        if not pd_targets:\n",
    "            if debug:\n",
    "                print(f\"DEBUG alpha_prior={alpha_prior:.4f} (no PD targets)\")\n",
    "            return float(alpha_prior)\n",
    "\n",
    "        alpha_mkt = Fit_Alpha_From_Pds_Multi(generator_base, pd_targets, rating_state, alpha_grid, weights=base_weights)\n",
    "\n",
    "        # Down-weight market term when confidence is low, so the prior can matter.\n",
    "        weights_eff = {T: base_weights.get(T, 1.0) * confidence for T in pd_targets.keys()}\n",
    "\n",
    "        # Stronger shrinkage when confidence is low (pull harder to peer prior)\n",
    "        lam = (lam0 / max(len(pd_targets), 1)) * (1.0 / confidence)\n",
    "\n",
    "        # Constrain search window: avoid extreme alpha jumps; widen window as confidence falls\n",
    "        band_eff = min(0.25, max(band0, band0 / confidence))\n",
    "        lo = min(alpha_mkt, alpha_prior) * (1.0 - band_eff)\n",
    "        hi = max(alpha_mkt, alpha_prior) * (1.0 + band_eff)\n",
    "\n",
    "        local_grid = alpha_grid[(alpha_grid >= lo) & (alpha_grid <= hi)]\n",
    "        if local_grid.size < 10:\n",
    "            local_grid = alpha_grid\n",
    "\n",
    "        alpha_map = Fit_Alpha_Map_Multi(\n",
    "            generator_base=generator_base,\n",
    "            pd_targets=pd_targets,\n",
    "            rating_state=rating_state,\n",
    "            alpha_prior=float(alpha_prior),\n",
    "            alpha_grid=local_grid,\n",
    "            lam=float(lam),\n",
    "            weights=weights_eff,\n",
    "        )\n",
    "\n",
    "        if debug:\n",
    "            print(\n",
    "                f\"DEBUG alpha_prior={alpha_prior:.4f} alpha_mkt={alpha_mkt:.4f} \"\n",
    "                f\"conf={confidence:.2f} band={band_eff:.3f} lam={lam:.6f} alpha_map={alpha_map:.4f}\"\n",
    "            )\n",
    "\n",
    "        return float(alpha_map)\n",
    "\n",
    "    def Get_Rating_State(issuer: str) -> int:\n",
    "        idx = issuer_to_idx.get(issuer, None)\n",
    "        return int(rating_state_arr[idx]) if idx is not None else 1\n",
    "\n",
    "    known_issuers = set(map(str, issuer_names))\n",
    "    return Estimate_Alpha_Naive, Estimate_Alpha_Vector, Get_Rating_State, known_issuers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 — “Vector platform” layer: routing and precedent retrieval\n",
    "\n",
    "### Objective\n",
    "Given an inbound unstructured message, determine:\n",
    "1) which workflow to run (routing), and  \n",
    "2) which prior cases are most similar (precedent retrieval).\n",
    "\n",
    "### Mathematical framework\n",
    "\n",
    "#### Text embeddings\n",
    "Each message $m$ is embedded into a vector $e(m)\\in\\mathbb{R}^d$ using a sentence transformer.\n",
    "\n",
    "Embeddings are normalized such that similarity is inner product:\n",
    "$$\n",
    "\\mathrm{sim}(a,b) = e(a)^\\top e(b).\n",
    "$$\n",
    "\n",
    "#### Nearest-neighbor retrieval (FAISS)\n",
    "Given workflow descriptions $\\{w_j\\}$ and case texts $\\{c_k\\}$, retrieval selects top matches:\n",
    "$$\n",
    "\\hat j = \\arg\\max_j e(m)^\\top e(w_j),\n",
    "\\qquad\n",
    "\\hat k_1,\\hat k_2 = \\text{top-2 over } e(m)^\\top e(c_k).\n",
    "$$\n",
    "\n",
    "### What the code does / produces\n",
    "- Builds FAISS indices for:\n",
    "  - workflow routing labels/descriptions,\n",
    "  - historical cases.\n",
    "- For each inbound message:\n",
    "  - prints top workflow route and a runner-up,\n",
    "  - prints two most similar cases and similarity scores.\n",
    "\n",
    "### Why it is designed this way\n",
    "- It emulates a “routing layer” that decides what downstream processing is appropriate.\n",
    "- Precedent retrieval provides explainability and governance context (“similar cases were handled this way”).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 2 — Vector-like platform layer: routing + retrieval\n",
    "# ============================================================\n",
    "\n",
    "WORKFLOWS = [\n",
    "    {\"name\": \"CDS_TRADE_CAPTURE\", \"desc\": \"Capture CDS trade, extract terms, validate, generate booking payload.\"},\n",
    "    {\"name\": \"CDS_CURVE_CALIBRATION\", \"desc\": \"Calibrate/update CDS curve; handle missing tenors; assess liquidity; store snapshot.\"},\n",
    "    {\"name\": \"TRANSITION_TILT_UPDATE\", \"desc\": \"Link CDS-implied PD to transition model; fit/regularize alpha tilt; log evidence.\"},\n",
    "    {\"name\": \"CREDIT_RISK_EXCEPTION\", \"desc\": \"Handle anomalies: outlier quotes, stale markets, inconsistent curve, escalation and case mgmt.\"},\n",
    "]\n",
    "\n",
    "HISTORICAL_CASES = [\n",
    "    {\"case_id\": \"C001\", \"text\": \"ACME 5Y widened post-earnings; illiquid tape; used peer-regularized tilt; documented override.\"},\n",
    "    {\"case_id\": \"C002\", \"text\": \"BETA missing 1Y/3Y; inferred short end from similar issuers; curve smoothing applied; governance note filed.\"},\n",
    "    {\"case_id\": \"C003\", \"text\": \"GAMMA HY; regime stress; transition tilt increased consistent with indices; no exception raised.\"},\n",
    "    {\"case_id\": \"C004\", \"text\": \"DELTA quote spike vs peers; suspect bad tick; opened exception; requested broker validation.\"},\n",
    "]\n",
    "\n",
    "\n",
    "def Embed_Texts(model, texts: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return normalized float32 embeddings suitable for inner-product (cosine) search.\n",
    "    \"\"\"\n",
    "    return model.encode(texts, convert_to_numpy=True, normalize_embeddings=True).astype(\"float32\")\n",
    "\n",
    "\n",
    "def Build_Faiss_Ip_Index(embeddings: np.ndarray) -> faiss.IndexFlatIP:\n",
    "    \"\"\"\n",
    "    Build a FAISS inner-product index:\n",
    "      similarity(x, y) = x^T y (when embeddings are normalized, this is cosine similarity).\n",
    "    \"\"\"\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "\n",
    "def Extract_Message_Fields(message_text: str, known_issuers: Set[str]) -> Dict[str, object]:\n",
    "    \"\"\"\n",
    "    Normalize message fields and include parsed quotes when available.\n",
    "\n",
    "    Behavior:\n",
    "      - If quotes exist: tenor_years = max tenor; spread_bp = spread at max tenor; quotes included\n",
    "      - Else: fallback regex tenor/spread if present\n",
    "    \"\"\"\n",
    "    stop_tokens = {\n",
    "        \"NOTE\", \"MATRIX\", \"CASE\", \"CALIBRATION\", \"UPDATE\", \"SPREAD\", \"CURVE\", \"PEERS\",\n",
    "        \"CDS\", \"PD\", \"USD\", \"EUR\", \"GBP\", \"JPY\", \"PLEASE\", \"BUY\", \"SELL\"\n",
    "    }\n",
    "\n",
    "    tokens = re.findall(r\"\\b[A-Z][A-Z0-9_\\-]{2,}\\b\", message_text.upper())\n",
    "    issuer = next((tok for tok in tokens if tok not in stop_tokens and tok in known_issuers), \"UNKNOWN\")\n",
    "\n",
    "    ccy_match = re.search(r\"\\b(USD|EUR|GBP|JPY)\\b\", message_text.upper())\n",
    "    ccy = ccy_match.group(1) if ccy_match else \"USD\"\n",
    "\n",
    "    spreads_num = Parse_Spreads_By_Tenor(message_text)\n",
    "    quotes = {f\"{int(k)}Y\": float(v) for k, v in sorted(spreads_num.items())} if spreads_num else None\n",
    "\n",
    "    if spreads_num:\n",
    "        max_tenor = float(max(spreads_num.keys()))\n",
    "        tenor_years = int(max_tenor)\n",
    "        spread_bp = float(spreads_num[max_tenor])\n",
    "    else:\n",
    "        tenor_match = re.search(r\"(\\d+)\\s*Y\", message_text, re.IGNORECASE)\n",
    "        spread_match = re.search(r\"(\\d+(?:\\.\\d+)?)\\s*bp\", message_text, re.IGNORECASE)\n",
    "        tenor_years = int(tenor_match.group(1)) if tenor_match else None\n",
    "        spread_bp = float(spread_match.group(1)) if spread_match else None\n",
    "\n",
    "    out: Dict[str, object] = {\n",
    "        \"issuer\": issuer,\n",
    "        \"tenor_years\": tenor_years,\n",
    "        \"spread_bp\": spread_bp,\n",
    "        \"ccy\": ccy,\n",
    "        \"raw_text\": message_text,\n",
    "    }\n",
    "    if quotes is not None:\n",
    "        out[\"quotes\"] = quotes\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 — End-to-end orchestration: ingestion → calibration → tilt → governance → reporting\n",
    "\n",
    "### Objective\n",
    "Run a unified loop over inbound messages that:\n",
    "1) routes and retrieves precedents,  \n",
    "2) extracts structured fields and CDS quotes,  \n",
    "3) calibrates hazards and PD targets (or reuses memory),  \n",
    "4) estimates tilts (naive and vector) with governance gating,  \n",
    "5) reports final multi-horizon transition-model PDs.\n",
    "\n",
    "### Mathematical framework\n",
    "\n",
    "#### Field extraction\n",
    "From message text, extract:\n",
    "- issuer identifier,\n",
    "- currency,\n",
    "- quotes $\\{T \\mapsto s(T)\\}$, and summary fields:\n",
    "  - $\\text{tenor\\_years}=\\max(T)$ if quotes exist,\n",
    "  - $\\text{spread\\_bp}=s(\\max(T))$ if quotes exist.\n",
    "\n",
    "#### Confidence scoring (heuristic)\n",
    "A confidence scalar $c\\in(0,1]$ is computed based on:\n",
    "- whether quotes are fresh vs reused,\n",
    "- whether terms indicate illiquidity/staleness,\n",
    "- number of tenors observed.\n",
    "\n",
    "This confidence drives:\n",
    "- whether parameters are stored (governance),\n",
    "- prior strength in $\\alpha$ estimation,\n",
    "- clipping band around market-fit $\\alpha$.\n",
    "\n",
    "#### Governance gating / “memory”\n",
    "The script maintains issuer-level memory:\n",
    "- last hazard curve $(\\{T_k\\},\\{\\gamma_k\\})$,\n",
    "- last stored $\\alpha_{\\text{naive}}$ and $\\alpha_{\\text{vector}}$.\n",
    "\n",
    "Policy:\n",
    "1) If **exception**: do nothing.\n",
    "2) If **no new quotes** but curve and stored alphas exist: reuse (no drift).\n",
    "3) If **quotes exist**: compute alphas for reporting; store only if $c \\ge c_{\\min}$.\n",
    "4) If **no quotes and no curve memory**: fall back to rating mean / vector prior.\n",
    "\n",
    "#### Guardrail around market-fit alpha\n",
    "When quotes exist, vector alpha is clipped around the market-fit estimate:\n",
    "$$\n",
    "\\alpha_{\\text{vector}} \\leftarrow \\min\\Bigl(\\max(\\alpha_{\\text{vector}},\\;\\alpha_{\\text{mkt}}(1-b)),\\;\\alpha_{\\text{mkt}}(1+b)\\Bigr),\n",
    "$$\n",
    "with band $b=b(c)$ increasing as confidence decreases.\n",
    "\n",
    "### What the code does / produces\n",
    "For each inbound message, it prints a complete trace including:\n",
    "- routing choice + similar precedents,\n",
    "- extracted fields (including quotes if present),\n",
    "- CDS calibration (hazards) and PD targets (or reuse),\n",
    "- confidence,\n",
    "- $\\alpha_{\\text{naive}}$ and $\\alpha_{\\text{vector}}$ with governance note,\n",
    "- final $\\mathrm{PD}_{\\mathrm{TM}}(1Y)$, $\\mathrm{PD}_{\\mathrm{TM}}(3Y)$, $\\mathrm{PD}_{\\mathrm{TM}}(5Y)$ for both tilts.\n",
    "\n",
    "### Why it is designed this way\n",
    "- It demonstrates an integrated “platform-like” workflow: *interpret → route → calibrate → link models → govern → report*.\n",
    "- Memory reuse reduces churn and prevents parameter drift in illiquid markets.\n",
    "- Confidence gating enforces a basic model risk management control: low-quality quotes influence reporting but do not overwrite stored parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##################################################################################################\n",
      "# Unified Demo: multi-horizon CDS↔transition tilt, stabilized by vector prior, embedded in routing layer\n",
      "##################################################################################################\n",
      "\n",
      "==================================================================================================\n",
      "INBOUND: Buy 50mm ACME 5Y CDS @ 120bp USD. Please book and confirm.\n",
      "ROUTE: CDS_CURVE_CALIBRATION (score=0.386) | next: CDS_TRADE_CAPTURE (score=0.361)\n",
      "FIELDS: {'issuer': 'ACME', 'tenor_years': 5, 'spread_bp': 120.0, 'ccy': 'USD', 'quotes': {'5Y': 120.0}}\n",
      "SIMILAR CASES:\n",
      "  - C001 (sim=0.430): ACME 5Y widened post-earnings; illiquid tape; used peer-regularized tilt; documented override.\n",
      "  - C002 (sim=0.186): BETA missing 1Y/3Y; inferred short end from similar issuers; curve smoothing applied; governance note filed.\n",
      "CDS calibration: spreads=(5Y:120bp) => hazards(gamma)=[0.02]\n",
      "CDS-implied PD targets: PD(5Y)≈9.49%\n",
      "CDS-implied PD (flat-hazard extrapolation): PD(1Y)≈1.98%, PD(3Y)≈5.81%, PD(5Y)≈9.49%\n",
      "DATA QUALITY: confidence≈0.60\n",
      "LINK (Transition Tilt) RESULTS:\n",
      "  rating_state=2 (0=IG,1=BBB-ish,2=HY)\n",
      "  alpha_naive=0.166 | PD_TM(1Y)=2.04%, PD_TM(3Y)=5.90%, PD_TM(5Y)=9.46%\n",
      "  alpha_vector=0.182 | PD_TM(1Y)=2.24%, PD_TM(3Y)=6.44%, PD_TM(5Y)=10.30%\n",
      "  Note: Fresh market quotes => calibrated and stored alphas (confidence-gated).\n",
      "\n",
      "==================================================================================================\n",
      "INBOUND: Calibration note: ACME curve missing 1Y; 5Y=120bp; liquidity poor.\n",
      "ROUTE: CDS_CURVE_CALIBRATION (score=0.396) | next: TRANSITION_TILT_UPDATE (score=0.255)\n",
      "FIELDS: {'issuer': 'ACME', 'tenor_years': 5, 'spread_bp': 120.0, 'ccy': 'USD', 'quotes': {'5Y': 120.0}}\n",
      "SIMILAR CASES:\n",
      "  - C001 (sim=0.424): ACME 5Y widened post-earnings; illiquid tape; used peer-regularized tilt; documented override.\n",
      "  - C002 (sim=0.419): BETA missing 1Y/3Y; inferred short end from similar issuers; curve smoothing applied; governance note filed.\n",
      "CDS calibration: spreads=(5Y:120bp) => hazards(gamma)=[0.02]\n",
      "CDS-implied PD targets: PD(5Y)≈9.49%\n",
      "CDS-implied PD (flat-hazard extrapolation): PD(1Y)≈1.98%, PD(3Y)≈5.81%, PD(5Y)≈9.49%\n",
      "DATA QUALITY: confidence≈0.10\n",
      "LINK (Transition Tilt) RESULTS:\n",
      "  rating_state=2 (0=IG,1=BBB-ish,2=HY)\n",
      "  alpha_naive=0.166 | PD_TM(1Y)=2.04%, PD_TM(3Y)=5.90%, PD_TM(5Y)=9.46%\n",
      "  alpha_vector=0.196 | PD_TM(1Y)=2.41%, PD_TM(3Y)=6.90%, PD_TM(5Y)=11.00%\n",
      "  Note: Fresh market quotes (low confidence) => calibrated for reporting, but NOT stored (governance gating).\n",
      "\n",
      "==================================================================================================\n",
      "INBOUND: We need to update transition matrix tilt for ACME using latest 5Y spread; align PD to market.\n",
      "ROUTE: TRANSITION_TILT_UPDATE (score=0.556) | next: CDS_CURVE_CALIBRATION (score=0.248)\n",
      "FIELDS: {'issuer': 'ACME', 'tenor_years': 5, 'spread_bp': None, 'ccy': 'USD'}\n",
      "SIMILAR CASES:\n",
      "  - C001 (sim=0.504): ACME 5Y widened post-earnings; illiquid tape; used peer-regularized tilt; documented override.\n",
      "  - C003 (sim=0.309): GAMMA HY; regime stress; transition tilt increased consistent with indices; no exception raised.\n",
      "CDS calibration: MISSING spreads => reused last hazard curve\n",
      "CDS-implied PD targets: PD(1Y)≈1.98%, PD(3Y)≈5.81%, PD(5Y)≈9.49%\n",
      "DATA QUALITY: confidence≈0.60 (reused curve)\n",
      "LINK (Transition Tilt) RESULTS:\n",
      "  rating_state=2 (0=IG,1=BBB-ish,2=HY)\n",
      "  alpha_naive=0.166 | PD_TM(1Y)=2.04%, PD_TM(3Y)=5.90%, PD_TM(5Y)=9.46%\n",
      "  alpha_vector=0.182 | PD_TM(1Y)=2.24%, PD_TM(3Y)=6.44%, PD_TM(5Y)=10.30%\n",
      "  Note: Missing quotes => reused last hazard curve AND last stored alphas (no parameter drift).\n",
      "\n",
      "==================================================================================================\n",
      "INBOUND: Spread looks wrong: DELTA quoted 1200bp but peers ~140bp; suspect bad tick. Raise exception case.\n",
      "ROUTE: CREDIT_RISK_EXCEPTION (score=0.250) | next: TRANSITION_TILT_UPDATE (score=0.123)\n",
      "FIELDS: {'issuer': 'DELTA', 'tenor_years': None, 'spread_bp': 1200.0, 'ccy': 'USD'}\n",
      "SIMILAR CASES:\n",
      "  - C004 (sim=0.627): DELTA quote spike vs peers; suspect bad tick; opened exception; requested broker validation.\n",
      "  - C002 (sim=0.241): BETA missing 1Y/3Y; inferred short end from similar issuers; curve smoothing applied; governance note filed.\n",
      "ACTION: exception workflow — do not update curve/alpha; open case and validate the quote.\n",
      "\n",
      "==================================================================================================\n",
      "INBOUND: Update tilt for BETA but no reliable CDS quote today; use peers/regime-consistent prior and log evidence.\n",
      "ROUTE: TRANSITION_TILT_UPDATE (score=0.644) | next: CDS_CURVE_CALIBRATION (score=0.507)\n",
      "FIELDS: {'issuer': 'BETA', 'tenor_years': None, 'spread_bp': None, 'ccy': 'USD'}\n",
      "SIMILAR CASES:\n",
      "  - C004 (sim=0.438): DELTA quote spike vs peers; suspect bad tick; opened exception; requested broker validation.\n",
      "  - C001 (sim=0.326): ACME 5Y widened post-earnings; illiquid tape; used peer-regularized tilt; documented override.\n",
      "CDS calibration: MISSING spreads => PD targets unavailable (curve memory empty).\n",
      "DATA QUALITY: confidence≈0.60\n",
      "LINK (Transition Tilt) RESULTS:\n",
      "  rating_state=1 (0=IG,1=BBB-ish,2=HY)\n",
      "  alpha_naive=1.222 | PD_TM(1Y)=1.27%, PD_TM(3Y)=6.74%, PD_TM(5Y)=13.66%\n",
      "  alpha_vector=1.062 | PD_TM(1Y)=1.03%, PD_TM(3Y)=5.49%, PD_TM(5Y)=11.35%\n",
      "  Note: No quotes and no curve memory => naive rating-mean; vector peer-conditioned prior.\n",
      "\n",
      "==================================================================================================\n",
      "INBOUND: ACME curve: 1Y=80bp 3Y=105bp 5Y=120bp USD; please update curve + tilt.\n",
      "ROUTE: TRANSITION_TILT_UPDATE (score=0.397) | next: CDS_CURVE_CALIBRATION (score=0.396)\n",
      "FIELDS: {'issuer': 'ACME', 'tenor_years': 5, 'spread_bp': 120.0, 'ccy': 'USD', 'quotes': {'1Y': 80.0, '3Y': 105.0, '5Y': 120.0}}\n",
      "SIMILAR CASES:\n",
      "  - C001 (sim=0.462): ACME 5Y widened post-earnings; illiquid tape; used peer-regularized tilt; documented override.\n",
      "  - C002 (sim=0.453): BETA missing 1Y/3Y; inferred short end from similar issuers; curve smoothing applied; governance note filed.\n",
      "CDS calibration: spreads=(1Y:80bp, 3Y:105bp, 5Y:120bp) => hazards(gamma)=[0.0133, 0.0197, 0.0242]\n",
      "CDS-implied PD targets: PD(1Y)≈1.32%, PD(3Y)≈5.13%, PD(5Y)≈9.61%\n",
      "DATA QUALITY: confidence≈1.00\n",
      "LINK (Transition Tilt) RESULTS:\n",
      "  rating_state=2 (0=IG,1=BBB-ish,2=HY)\n",
      "  alpha_naive=0.160 | PD_TM(1Y)=1.97%, PD_TM(3Y)=5.69%, PD_TM(5Y)=9.14%\n",
      "  alpha_vector=0.160 | PD_TM(1Y)=1.97%, PD_TM(3Y)=5.69%, PD_TM(5Y)=9.14%\n",
      "  Note: Fresh market quotes => calibrated and stored alphas (confidence-gated).\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SECTION 3 — Unified end-to-end run (multi-horizon linkage)\n",
    "# ============================================================\n",
    "\n",
    "def Main():\n",
    "    # ------------------------------------------------------------\n",
    "    # 0) Transition model base (annual TM -> generator_base)\n",
    "    # ------------------------------------------------------------\n",
    "    _, generator_base = Build_Q_Base_From_Transition_Data(use_estimated=True)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 1) Universe + alpha estimators\n",
    "    # ------------------------------------------------------------\n",
    "    universe = Build_Structured_Issuer_Universe(generator_base=generator_base, seed=7, num_issuers=400)\n",
    "    estimate_alpha_naive, estimate_alpha_vector, get_rating_state, known_issuers = Build_Alpha_Estimators(generator_base, universe)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 2) Routing + precedent retrieval indices\n",
    "    # ------------------------------------------------------------\n",
    "    model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    workflow_texts = [f\"{w['name']}: {w['desc']}\" for w in WORKFLOWS]\n",
    "    workflow_index = Build_Faiss_Ip_Index(Embed_Texts(model, workflow_texts))\n",
    "\n",
    "    case_texts = [c[\"text\"] for c in HISTORICAL_CASES]\n",
    "    case_index = Build_Faiss_Ip_Index(Embed_Texts(model, case_texts))\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 3) CDS inputs\n",
    "    # ------------------------------------------------------------\n",
    "    discount_curve = Flat_Discount_Curve(rate_r=0.03)\n",
    "    recovery_r = 0.40\n",
    "\n",
    "    inbound_messages = [\n",
    "        \"Buy 50mm ACME 5Y CDS @ 120bp USD. Please book and confirm.\",\n",
    "        \"Calibration note: ACME curve missing 1Y; 5Y=120bp; liquidity poor.\",\n",
    "        \"We need to update transition matrix tilt for ACME using latest 5Y spread; align PD to market.\",\n",
    "        \"Spread looks wrong: DELTA quoted 1200bp but peers ~140bp; suspect bad tick. Raise exception case.\",\n",
    "        \"Update tilt for BETA but no reliable CDS quote today; use peers/regime-consistent prior and log evidence.\",\n",
    "        \"ACME curve: 1Y=80bp 3Y=105bp 5Y=120bp USD; please update curve + tilt.\",\n",
    "    ]\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 4) Memory: last good curve + last approved alphas\n",
    "    # ------------------------------------------------------------\n",
    "    last_hazards: Dict[str, Tuple[List[float], List[float]]] = {}\n",
    "    last_alpha_naive: Dict[str, float] = {}\n",
    "    last_alpha_vector: Dict[str, float] = {}\n",
    "\n",
    "    report_horizons = [1.0, 3.0, 5.0]\n",
    "\n",
    "    # Governance thresholds\n",
    "    conf_store_min = 0.50\n",
    "    conf_reused_curve = 0.60\n",
    "\n",
    "    # Guardrail around market-fit alpha when quote exists\n",
    "    band_min = 0.03\n",
    "    band_max = 0.20\n",
    "\n",
    "    def Band_From_Confidence(confidence: float) -> float:\n",
    "        return float(band_min + (1.0 - confidence) * (band_max - band_min))\n",
    "\n",
    "    def Confidence_From_Message(\n",
    "        msg: str,\n",
    "        fresh_market: bool,\n",
    "        reused_curve: bool,\n",
    "        num_tenors: int,\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Heuristic confidence score:\n",
    "          - reused curve => fixed medium confidence\n",
    "          - if market quote exists, start at baseline and penalize illiquidity/missing keywords\n",
    "          - reward more tenors (more constraints => more stable hazard curve)\n",
    "        \"\"\"\n",
    "        if reused_curve:\n",
    "            return float(conf_reused_curve)\n",
    "        if not fresh_market:\n",
    "            return 0.60\n",
    "\n",
    "        conf = 0.60\n",
    "\n",
    "        if re.search(r\"\\billiquid\\b|\\bliquidity\\s+poor\\b|\\bstale\\b|\\bthin\\b\", msg, flags=re.IGNORECASE):\n",
    "            conf *= 0.25\n",
    "        if re.search(r\"\\bmissing\\b\", msg, flags=re.IGNORECASE):\n",
    "            conf *= 0.70\n",
    "\n",
    "        if num_tenors >= 2:\n",
    "            conf = min(1.0, conf + 0.20)\n",
    "        if num_tenors >= 3:\n",
    "            conf = min(1.0, conf + 0.20)\n",
    "\n",
    "        return float(np.clip(conf, 0.05, 1.00))\n",
    "\n",
    "    print(\"\\n\" + \"#\" * 98)\n",
    "    print(\"# Unified Demo: multi-horizon CDS↔transition tilt, stabilized by vector prior, embedded in routing layer\")\n",
    "    print(\"#\" * 98)\n",
    "\n",
    "    for msg in inbound_messages:\n",
    "        # ----------------------------\n",
    "        # A) Route\n",
    "        # ----------------------------\n",
    "        query_emb = Embed_Texts(model, [msg])\n",
    "        wf_scores, wf_ids = workflow_index.search(query_emb, k=2)\n",
    "\n",
    "        chosen_workflow = WORKFLOWS[int(wf_ids[0][0])]\n",
    "        alt_workflow = WORKFLOWS[int(wf_ids[0][1])]\n",
    "\n",
    "        # ----------------------------\n",
    "        # B) Extract structured fields\n",
    "        # ----------------------------\n",
    "        fields = Extract_Message_Fields(msg, known_issuers)\n",
    "        issuer = str(fields[\"issuer\"])\n",
    "        rating_state = int(get_rating_state(issuer))\n",
    "\n",
    "        # ----------------------------\n",
    "        # C) Retrieve similar cases\n",
    "        # ----------------------------\n",
    "        case_scores, case_ids = case_index.search(query_emb, k=2)\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 98)\n",
    "        print(\"INBOUND:\", msg)\n",
    "        print(f\"ROUTE: {chosen_workflow['name']} (score={wf_scores[0][0]:.3f}) | next: {alt_workflow['name']} (score={wf_scores[0][1]:.3f})\")\n",
    "\n",
    "        printable_fields = {k: fields[k] for k in [\"issuer\", \"tenor_years\", \"spread_bp\", \"ccy\"] if k in fields}\n",
    "        if \"quotes\" in fields:\n",
    "            printable_fields[\"quotes\"] = fields[\"quotes\"]\n",
    "        print(\"FIELDS:\", printable_fields)\n",
    "\n",
    "        print(\"SIMILAR CASES:\")\n",
    "        for rank in range(2):\n",
    "            case = HISTORICAL_CASES[int(case_ids[0][rank])]\n",
    "            print(f\"  - {case['case_id']} (sim={case_scores[0][rank]:.3f}): {case['text']}\")\n",
    "\n",
    "        # ----------------------------\n",
    "        # D) Exception gating\n",
    "        # ----------------------------\n",
    "        if chosen_workflow[\"name\"] == \"CREDIT_RISK_EXCEPTION\":\n",
    "            print(\"ACTION: exception workflow — do not update curve/alpha; open case and validate the quote.\")\n",
    "            continue\n",
    "\n",
    "        # ----------------------------\n",
    "        # E) CDS curve calibration / PD targets\n",
    "        # ----------------------------\n",
    "        spreads = Parse_Spreads_By_Tenor(msg)\n",
    "\n",
    "        # If no explicit quotes were parsed, fall back to extracted tenor/spread (if present).\n",
    "        if not spreads and fields.get(\"spread_bp\") is not None and fields.get(\"tenor_years\") is not None:\n",
    "            spreads = {float(fields[\"tenor_years\"]): float(fields[\"spread_bp\"])}\n",
    "\n",
    "        fresh_market = False\n",
    "        reused_curve = False\n",
    "        pd_targets: Optional[Dict[float, float]] = None\n",
    "\n",
    "        if spreads:\n",
    "            fresh_market = True\n",
    "            tenors, gammas = Calibrate_Piecewise_Hazards(spreads, discount_curve=discount_curve, recovery_r=recovery_r)\n",
    "            last_hazards[issuer] = (tenors, gammas)\n",
    "\n",
    "            # PD targets only at quoted tenors; (the reporting horizons are handled below when needed)\n",
    "            pd_targets = {float(T): Cds_Pd_From_Hazards(tenors, gammas, float(T)) for T in sorted(spreads.keys())}\n",
    "\n",
    "            curve_str = \", \".join([f\"{int(t)}Y:{spreads[t]:.0f}bp\" for t in sorted(spreads)])\n",
    "            print(f\"CDS calibration: spreads=({curve_str}) => hazards(gamma)={np.round(gammas, 4).tolist()}\")\n",
    "\n",
    "            if len(spreads) == 1:\n",
    "                only_T = float(next(iter(spreads.keys())))\n",
    "                print(f\"CDS-implied PD targets: PD({int(only_T)}Y)≈{pd_targets[only_T]:.2%}\")\n",
    "\n",
    "                # Flat-hazard extrapolation in this demo is implicit: last gamma extends beyond last tenor.\n",
    "                pd_extrap = {T: Cds_Pd_From_Hazards(tenors, gammas, float(T)) for T in report_horizons}\n",
    "                print(\"CDS-implied PD (flat-hazard extrapolation): \" + \", \".join([f\"PD({int(T)}Y)≈{pd_extrap[T]:.2%}\" for T in report_horizons]))\n",
    "            else:\n",
    "                print(\"CDS-implied PD targets: \" + \", \".join([f\"PD({int(T)}Y)≈{pd_targets[T]:.2%}\" for T in sorted(pd_targets.keys())]))\n",
    "\n",
    "        else:\n",
    "            if issuer in last_hazards:\n",
    "                reused_curve = True\n",
    "                tenors, gammas = last_hazards[issuer]\n",
    "\n",
    "                # When reusing curve, build PD targets for reporting horizons (1/3/5)\n",
    "                pd_targets = {float(T): Cds_Pd_From_Hazards(tenors, gammas, float(T)) for T in report_horizons}\n",
    "\n",
    "                print(\"CDS calibration: MISSING spreads => reused last hazard curve\")\n",
    "                print(\"CDS-implied PD targets: \" + \", \".join([f\"PD({int(T)}Y)≈{pd_targets[T]:.2%}\" for T in report_horizons]))\n",
    "            else:\n",
    "                print(\"CDS calibration: MISSING spreads => PD targets unavailable (curve memory empty).\")\n",
    "\n",
    "        # ----------------------------\n",
    "        # F) Confidence score\n",
    "        # ----------------------------\n",
    "        conf = Confidence_From_Message(\n",
    "            msg,\n",
    "            fresh_market=fresh_market,\n",
    "            reused_curve=reused_curve,\n",
    "            num_tenors=len(spreads) if spreads else 0,\n",
    "        )\n",
    "        conf_note = \" (reused curve)\" if reused_curve else \"\"\n",
    "        print(f\"DATA QUALITY: confidence≈{conf:.2f}{conf_note}\")\n",
    "\n",
    "        # ----------------------------\n",
    "        # G) Alpha estimation policy (governance + memory)\n",
    "        # ----------------------------\n",
    "        if reused_curve and (issuer in last_alpha_naive) and (issuer in last_alpha_vector):\n",
    "            alpha_naive = float(last_alpha_naive[issuer])\n",
    "            alpha_vector = float(last_alpha_vector[issuer])\n",
    "            alpha_note = \"Missing quotes => reused last hazard curve AND last stored alphas (no parameter drift).\"\n",
    "\n",
    "        elif pd_targets:\n",
    "            # Naive is market-fit; used as \"mkt\" reference for banding\n",
    "            alpha_naive = float(estimate_alpha_naive(issuer, pd_targets))\n",
    "            alpha_mkt = float(alpha_naive)\n",
    "\n",
    "            alpha_vector_raw = float(estimate_alpha_vector(issuer, pd_targets, confidence=conf))\n",
    "\n",
    "            # Clip to band around market-fit alpha to remain market-respecting.\n",
    "            band = Band_From_Confidence(conf)\n",
    "            lo, hi = alpha_mkt * (1.0 - band), alpha_mkt * (1.0 + band)\n",
    "            alpha_vector = float(np.clip(alpha_vector_raw, lo, hi))\n",
    "\n",
    "            # Store only if confidence passes governance threshold.\n",
    "            if conf >= conf_store_min:\n",
    "                last_alpha_naive[issuer] = alpha_naive\n",
    "                last_alpha_vector[issuer] = alpha_vector\n",
    "                alpha_note = \"Fresh market quotes => calibrated and stored alphas (confidence-gated).\"\n",
    "            else:\n",
    "                alpha_note = \"Fresh market quotes (low confidence) => calibrated for reporting, but NOT stored (governance gating).\"\n",
    "\n",
    "        else:\n",
    "            # No PD targets => fall back (naive: rating mean; vector: peer prior)\n",
    "            alpha_naive = float(estimate_alpha_naive(issuer, None))\n",
    "            alpha_vector = float(estimate_alpha_vector(issuer, None, confidence=conf))\n",
    "            alpha_note = \"No quotes and no curve memory => naive rating-mean; vector peer-conditioned prior.\"\n",
    "\n",
    "        # ----------------------------\n",
    "        # H) Report PD_TM under both alphas\n",
    "        # ----------------------------\n",
    "        pd_tm_naive = {T: Pd_Tm(generator_base, alpha_naive, float(T), rating_state) for T in report_horizons}\n",
    "        pd_tm_vector = {T: Pd_Tm(generator_base, alpha_vector, float(T), rating_state) for T in report_horizons}\n",
    "\n",
    "        print(\"LINK (Transition Tilt) RESULTS:\")\n",
    "        print(f\"  rating_state={rating_state} (0=IG,1=BBB-ish,2=HY)\")\n",
    "        print(f\"  alpha_naive={alpha_naive:.3f} | \" + \", \".join([f\"PD_TM({int(T)}Y)={pd_tm_naive[T]:.2%}\" for T in report_horizons]))\n",
    "        print(f\"  alpha_vector={alpha_vector:.3f} | \" + \", \".join([f\"PD_TM({int(T)}Y)={pd_tm_vector[T]:.2%}\" for T in report_horizons]))\n",
    "        print(f\"  Note: {alpha_note}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    Main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to read the printed output\n",
    "\n",
    "For each inbound message, the notebook prints:\n",
    "\n",
    "- **ROUTE**: which workflow the text embedding selected and the similarity score.\n",
    "- **FIELDS**: extracted issuer/tenor/spread/currency.\n",
    "- **SIMILAR CASES**: most similar precedents by text embedding similarity.\n",
    "- **CDS→PD proxy**: computed $\\lambda$ and $PD_{\\mathrm{CDS}}(5Y)$ when spread exists.\n",
    "- **LINK (Transition Tilt) RESULTS**:\n",
    "  - `alpha_naive` and resulting $PD_{\\mathrm{TM}}(1Y)$, $PD_{\\mathrm{TM}}(5Y)$,\n",
    "  - `alpha_vector` and resulting $PD_{\\mathrm{TM}}(1Y)$, $PD_{\\mathrm{TM}}(5Y)$,\n",
    "  - notes indicating whether the system used market calibration, memory reuse, or peer prior.\n",
    "\n",
    "The intended takeaway is:\n",
    "- With reliable quotes, vector calibration stays close to market but reduces instability.\n",
    "- With missing quotes, vectors enable peer-conditioned priors and better defaults than rating means.\n",
    "- With exceptions, the workflow layer prevents erroneous updates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
